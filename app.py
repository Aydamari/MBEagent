# --- START OF FILE app.py ---

import streamlit as st
import google.generativeai as genai
import PyPDF2 as pdf
import os

# --- FUN√á√ïES AUXILIARES ---

@st.cache_data
def load_prompt(file_path):
    """L√™ e retorna o conte√∫do de um arquivo de texto."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        st.error(f"ERRO: O arquivo de prompt '{file_path}' n√£o foi encontrado. Certifique-se de que ele est√° no mesmo diret√≥rio que o app.py.")
        return None

def extract_text_from_pdf(uploaded_file):
    """Extrai texto de um arquivo PDF enviado."""
    if uploaded_file is not None:
        try:
            pdf_reader = pdf.PdfReader(uploaded_file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text()
            return text
        except Exception as e:
            st.error(f"Erro ao ler o PDF: {e}")
            return None
    return None

# --- CONFIGURA√á√ÉO INICIAL E CARREGAMENTO DE DADOS ---

PROMPT_MESTRE = load_prompt("master_prompt.txt")

try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=API_KEY)
except (KeyError, FileNotFoundError):
    st.error("Chave de API do Google n√£o configurada! Adicione-a nos segredos (secrets) do seu app no Streamlit Cloud.")
    st.stop()

# --- INTERFACE DO USU√ÅRIO (UI) ---

st.set_page_config(page_title="Analisador de Artigos MBE/PBE", layout="wide")

# <<< MUDAN√áA: CSS MAIS ROBUSTO >>>
st.markdown("""
<style>
    /* Seletores mais espec√≠ficos para o Streamlit */
    [data-testid="stMarkdown"] h1,
    [data-testid="stMarkdown"] h2,
    [data-testid="stMarkdown"] h3 {
        color: black !important; /* For√ßa a cor preta nos cabe√ßalhos */
        font-weight: bold !important;
    }
    /* Estilos para a tabela ser responsiva */
    [data-testid="stMarkdown"] table {
        width: 100% !important;
        table-layout: fixed !important; /* Impede a tabela de estourar o container */
    }
    [data-testid="stMarkdown"] th,
    [data-testid="stMarkdown"] td {
        word-wrap: break-word !important; /* For√ßa a quebra de linha em palavras longas */
        overflow-wrap: break-word !important;
    }
    [data-testid="stMarkdown"] th {
        background-color: #f0f2f6 !important; /* Fundo suave para o cabe√ßalho da tabela */
        color: black !important;
        font-weight: bold !important;
    }
</style>
""", unsafe_allow_html=True)

st.header("üî¨ Analisador de Artigos Cient√≠ficos MBE/PBE")
st.caption("Desenvolvido por Igor Eckert & Aydamari Faria-Jr")

# Barra Lateral com Configura√ß√µes
st.sidebar.title("Configura√ß√µes")

model_mapping = {
    "Gemini 2.0 Flash": "gemini-2.0-flash",
    "Gemini 2.5 Flash": "gemini-2.5-flash-preview-04-17",
    "Gemini 2.5 Pro (Em breve)": "disabled"
}
model_options = list(model_mapping.keys())

selected_model_name = st.sidebar.selectbox(
    "Escolha o modelo de IA:", options=model_options, index=1
)
st.sidebar.info("A sele√ß√£o do modelo impacta a velocidade e a qualidade da an√°lise. O Gemini 2.5 Pro ser√° habilitado no futuro.")

# Interface Principal
prompt_usuario = st.text_area(
    "Instru√ß√µes Adicionais (Opcional):",
    height=100,
    placeholder="Analise o artigo em anexo."
)
uploaded_file = st.file_uploader("Fa√ßa o upload do seu artigo em PDF aqui:", type=["pdf"])
submit_button = st.button("Analisar Artigo")

# --- L√ìGICA PRINCIPAL ---

if PROMPT_MESTRE and submit_button:
    actual_model_id = model_mapping[selected_model_name]

    if actual_model_id == "disabled":
        st.warning("O modelo Gemini 2.5 Pro ainda n√£o est√° dispon√≠vel. Por favor, selecione outro modelo.")
    elif uploaded_file is None:
        st.warning("Por favor, fa√ßa o upload de um arquivo PDF antes de analisar.")
    else:
        with st.spinner("Extraindo texto do PDF..."):
            texto_extraido = extract_text_from_pdf(uploaded_file)

        if texto_extraido:
            st.info(f"Texto extra√≠do com sucesso! Enviando para o modelo: **{selected_model_name}**")

            prompt_final = f"{PROMPT_MESTRE}\n---\nINSTRU√á√ÉO ADICIONAL DO USU√ÅRIO (se houver, deve complementar a tarefa principal):\n{prompt_usuario if prompt_usuario else 'Nenhuma instru√ß√£o adicional fornecida.'}\n---\nCONTE√öDO DO ARTIGO CIENT√çFICO PARA AN√ÅLISE:\n{texto_extraido}"

            try:
                with st.spinner(f"O modelo '{selected_model_name}' est√° processando a an√°lise cr√≠tica..."):
                    model = genai.GenerativeModel(actual_model_id)
                    response = model.generate_content(prompt_final)
                
                # <<< MUDAN√áA: T√≠tulo agora √© gerado apenas pelo app >>>
                st.subheader("Resultado da An√°lise Cr√≠tica")
                with st.container(border=True):
                    st.markdown(response.text)

            except Exception as e:
                st.error(f"Ocorreu um erro ao chamar a API do Gemini: {e}")
